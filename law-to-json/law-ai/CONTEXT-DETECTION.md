# D√©tection Automatique du Contexte IA

## Vue d'ensemble

Le syst√®me d√©tecte automatiquement la taille du contexte disponible selon le provider IA actif (Ollama ou Groq) et ajuste dynamiquement la taille des chunks pour optimiser le traitement.

## Fonctionnement

### 1. D√©tection du contexte

```java
// IAService.getMaxContextTokens() interroge le provider actif
int contextTokens = iaService.getMaxContextTokens();
// Retourne: 8192 pour Ollama gemma, 32768 pour Groq llama-3.3-70b
```

### 2. Calcul dynamique

**Formule** : `maxChunkSize = (contextTokens √ó ratio) √ó 4`

- **contextTokens** : Taille du contexte du mod√®le (ex: 8192, 32768)
- **ratio** : Ratio de s√©curit√© (0.7 = 70% du contexte)
- **4** : Approximation chars/token

### 3. Exemples de calcul

| Provider | Mod√®le | Context | Ratio | Max Chunk |
|----------|--------|---------|-------|-----------|
| Ollama | gemma | 8,192 tokens | 70% | ~23K chars |
| Ollama | mixtral | 32,768 tokens | 70% | ~92K chars |
| Groq | llama-3.3-70b | 32,768 tokens | 70% | ~92K chars |
| Groq | llama-3.2-90b-vision | 8,192 tokens | 70% | ~23K chars |
| Fallback | N/A | N/A | N/A | 8,000 chars |

## Configuration

```yaml
# application.yml
batch:
  ai:
    max-chunk-size: 8000          # Fallback si contexte inconnu
    context-usage-ratio: 0.7      # Utiliser 70% du contexte
    chunk-overlap: 200            # Overlap entre chunks
```

### Param√®tres

- **max-chunk-size** : Valeur de fallback si le provider ne fournit pas d'info de contexte
- **context-usage-ratio** : Pourcentage du contexte √† utiliser (0.0 √† 1.0)
  - **0.7** recommand√© : garde 30% de marge pour les m√©tadonn√©es et la r√©ponse
  - **0.8** agressif : utilise plus de contexte, risque de d√©passement
  - **0.5** conservateur : s√©curitaire mais sous-utilise le contexte
- **chunk-overlap** : Nombre de caract√®res de chevauchement entre chunks

## Avantages

1. **Auto-adaptation** : S'ajuste automatiquement selon le mod√®le utilis√©
2. **Performance optimale** : Utilise au maximum le contexte disponible
3. **S√©curit√©** : Ratio de 70% √©vite les d√©passements
4. **Flexibilit√©** : Passe d'Ollama (8K) √† Groq (32K) sans configuration
5. **Graceful degradation** : Fallback sur valeur par d√©faut si d√©tection √©choue

## Workflow

```
Document OCR (50K chars)
    ‚Üì
AiProcessor.process()
    ‚Üì
calculateMaxChunkSize()
    ‚Üì d√©tecte provider actif
    ‚Üì
Groq llama-3.3-70b d√©tect√©: 32768 tokens
    ‚Üì calcul: 32768 √ó 0.7 √ó 4 = 91,750 chars
    ‚Üì
needsChunking(50K, 91K) = false
    ‚Üì 50K < 91K ‚Üí pas de chunking n√©cessaire
    ‚Üì
processSingleText() ‚Üí traitement direct
```

## Cas d'usage

### Petit document (< contexte)
- **D√©tection** : 5,000 chars, contexte 23K chars
- **Action** : Traitement direct sans chunking
- **Avantage** : Pas de d√©coupe inutile, contexte pr√©serv√©

### Document moyen (‚âà contexte)
- **D√©tection** : 80,000 chars, contexte 92K chars (Groq)
- **Action** : Traitement direct sans chunking
- **Avantage** : Exploite pleinement le grand contexte de Groq

### Gros document (> contexte)
- **D√©tection** : 150,000 chars, contexte 23K chars (Ollama)
- **Action** : D√©coupage en ~7 chunks de 23K chars
- **Avantage** : Permet le traitement malgr√© contexte limit√©

## Logs

```
üìä Contexte d√©tect√©: 32768 tokens ‚Üí max chunk: 91750 chars (ratio: 70%)
ü§ñ Am√©lioration IA pour decret-2024-150 (OCR: 85432 chars)
‚úÖ Document trait√© sans chunking (85K < 92K)
```

```
üìä Contexte d√©tect√©: 8192 tokens ‚Üí max chunk: 22937 chars (ratio: 70%)
ü§ñ Am√©lioration IA pour loi-2024-025 (OCR: 156789 chars)
üì¶ Document trop volumineux (156789 chars), d√©coupage en chunks (max: 22937)
‚úÖ Text chunked: 156789 chars ‚Üí 7 chunks
```

## Code

### IAService.java
```java
/**
 * Retourne la taille maximale du contexte du provider actif (en tokens).
 */
int getMaxContextTokens();
```

### IAServiceImpl.java
```java
@Override
public int getMaxContextTokens() {
    IAProvider provider = providerFactory.selectProvider(false, 1000);
    Optional<ModelInfo> modelInfo = provider.selectBestModel(false, 1000);
    return modelInfo.isPresent() ? modelInfo.get().contextWindow() 
                                  : provider.getCapabilities().maxContextTokens();
}
```

### AiProcessor.java
```java
private int calculateMaxChunkSize() {
    int contextTokens = iaService.getMaxContextTokens();
    if (contextTokens <= 0) {
        return fallbackMaxChunkSize;
    }
    return (int) (contextTokens * contextUsageRatio * 4);
}
```

## Tests

‚úÖ Tous les tests d'int√©gration passent (5/5)
‚úÖ D√©tection automatique fonctionnelle
‚úÖ Fallback op√©rationnel si provider non disponible
‚úÖ Chunking adaptatif selon contexte d√©tect√©
